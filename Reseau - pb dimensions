import numpy as np
import time
from scipy import misc, linalg
from random import randrange, shuffle
import pickle
import sys
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image
from sklearn.utils import shuffle
from sklearn.datasets import fetch_mldata
from sklearn.metrics import confusion_matrix
from sklearn.cluster import KMeans
from sklearn.externals import joblib

#path="http://www.math.univ-toulouse.fr/~besse/Wikistat/data/"
#Dtrain=pd.read_csv(path+"mnist_train.csv",header=None)
#Dtrain.head()
#print(Dtrain.shape)

DATA_PATH = 'data'

def load_mnist_data():
    """Easy way to fetch and prepare mnist data."""
    mnist = fetch_mldata('MNIST original', data_home=DATA_PATH)

    # Dans MNIST, les données sont triées par labels (les 0 d'abord, les 1
    # ensuite…), ce qui ne nous convient pas. Mélangeons-les.
    X, y = shuffle(mnist.data, mnist.target)

    # X est une matrice de taille(70000, 784)
    # X[0] est la première image de la liste
    # X[0][0] est le premier pixel de cette image
    # y est une matrice de taille (70000,)
    # y[0] est la valeur représentée par l'image X[0]

    # Comme les valeurs des pixels sont exprimées entre 0 et 255, nous divisons
    # par 255 pour obtenir des valeurs comprises entre 0 et 1.
    return X / 255.0, y


def sigmoid_prime(z):
    return sigmoid(z) * (1 - sigmoid(z))

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def to_one_hot(y, k):

    one_hot = np.zeros(k)
    one_hot[y] = 1
    return one_hot



class Layer:
    def __init__(self, dim, input_size):
        self.dim = dim
        self.input_size = input_size
        self.weights = np.random.randn(dim, input_size)
        self.biases = np.random.randn(dim)
        self.z = np.array([], float)
        self.a = np.array([], float)

    def output(self, data):
        print('shape weights')
        print(np.shape(self.weights))
        self.z = np.dot(self.weights, data) + self.biases
        print('shape z (z = np.dot(self.weights, data) + self.biases)')
        print(np.shape(self.z))
        return self.z

    def forward(self, data): #predict
        self.a = sigmoid(self.output(data))
        return self.a

    def majweight(self, grad, rate, i, j):
        self.weights[i, j] -= rate * grad

    def majbiase(self, grad, rate):
        self.biases -= rate * grad


class Network:

    def __init__(self, input_dim):
        self.layers = []
        self.input_dim = input_dim

    def propagation(self, input_data):
        activation = input_data
        for layer in self.layers:
            activation = layer.forward(activation)
        return activation

    def add_layer(self, size):
        if len(self.layers) > 0:
            input_dim = self.layers[-1].dim
        else:
            input_dim = self.input_dim

        self.layers.append(Layer(size, input_dim))

    def predict(self, input_data):
        return np.argmax(self.propagation(input_data))

        # Évalue la performance du réseau à partir d'un set d'exemples.
        # Retourne un nombre entre 0 et 1.

    def erreur(self, data, exemple):
        d = exemple
        p = data
        return max(np.matrix(d) - np.matrix(self.propagation(p)))




    def train(self, data, exemple, rate):

        deltas = []
        delta = self.output_delta(data, exemple)
        deltas.append(delta)


        nb_layers = len(self.layers)
        for l in reversed(range(nb_layers - 1)):
            layer = self.layers[l]
            next_layer = self.layers[l + 1]

            print('shape layer')
            print (np.shape(next_layer.weights.transpose()))

            print('shape delta')
            print(np.shape(delta))

            delta = sigmoid_prime(layer.z) * np.dot(next_layer.weights.transpose(), delta)
            deltas.append(delta)

        deltas = list(reversed(deltas))

        for layer in self.layers:
            index = self.layers.index(layer)
            prc_layer = None
            if index > 0:
                prc_layer = self.layers[index - 1]
            for j in range(layer.dim-1):
                for k in range(layer.input_size):
                    if index == 0:
                        layer.majweight(data[k] * deltas[index][j], rate, j, k)
                    else:
                        layer.majweight(prc_layer.a[k] * deltas[index][j], rate, j, k)
                layer.majbiase(deltas[index], rate)

    def output_delta(self, data, target):
        out = self.propagation(data)
        return out-target







if __name__ == '__main__':

    # Pour transformer une image en matrice
    #data = np.array(image.png)
    #soit
    #niveau de gris 
    #img = Image.open('data.png').convert('LA')
    #img.save('data1.png')

    data = mpimg.imread('data3.png')

    shape = data.shape

    # make a 1-dimensional view of arr
    flat_arr = data.ravel()

    # convert it to a matrix
    vector = np.matrix(flat_arr)


    # redimensionne matrice
    data2 = np.asarray(vector).reshape(1,-1)
    print('dim image')
    print(np.shape(data2))

    data2 = np.transpose(data2)

    exemple = mpimg.imread('exemple.png')


    print('- - - - - - - - - - - ')

    net = Network(576)
    net.add_layer(200)
    net.add_layer(1)

    train_rate = 0.05
    for i in range(1, 15000):
        net.train(data2, data2, train_rate)

    print(net.propagation(data2))
