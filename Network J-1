import numpy as np
import time
from scipy import misc, linalg, random
from random2 import shuffle
import pickle
import sys
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image
import cv2
from sklearn.utils import shuffle
from sklearn.datasets import fetch_mldata
from sklearn.metrics import confusion_matrix
from sklearn.cluster import KMeans
from sklearn.externals import joblib

#path="http://www.math.univ-toulouse.fr/~besse/Wikistat/data/"
#Dtrain=pd.read_csv(path+"mnist_train.csv",header=None)
#Dtrain.head()
#print(Dtrain.shape)

DATA_PATH = 'data'

def load_mnist_data():
    """Easy way to fetch and prepare mnist data."""
    mnist = fetch_mldata('MNIST original', data_home=DATA_PATH)

    # Dans MNIST, les données sont triées par labels (les 0 d'abord, les 1
    # ensuite…), ce qui ne nous convient pas. Mélangeons-les.
    X, y = shuffle(mnist.data, mnist.target)

    # X est une matrice de taille(70000, 784)
    # X[0] est la première image de la liste
    # X[0][0] est le premier pixel de cette image
    # y est une matrice de taille (70000,)
    # y[0] est la valeur représentée par l'image X[0]

    # Comme les valeurs des pixels sont exprimées entre 0 et 255, nous divisons
    # par 255 pour obtenir des valeurs comprises entre 0 et 1.
    return X / 255.0, y


def sigmoid_prime(z):
    return sigmoid(z) * (1 - sigmoid(z))

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def to_one_hot(y, k):

    one_hot = np.zeros(k)
    one_hot[y] = 1
    return one_hot



class Layer:
    def __init__(self, dim, input_size):
        self.dim = dim
        self.input_size = input_size
        self.weights = np.random.randn(dim, input_size)
        self.biases = np.random.randn(dim)
        self.z = np.array([], float)
        self.a = np.array([], float)

    def output(self, data):
        self.z = np.dot(self.weights, data) + self.biases
        return self.z

    def forward(self, data): #predict
        self.a = sigmoid(self.output(data))
        return self.a

    def majweight(self, grad, rate, i, j):
        self.weights[i, j] -= rate * grad

    def majbiase(self, grad, rate):
        self.biases -= rate * grad


class Network:

    def __init__(self, input_dim):
        self.layers = []
        self.input_dim = input_dim

    def propagation(self, input_data):
        activation = input_data
        for layer in self.layers:
            activation = layer.forward(activation)
        return activation

    def add_layer(self, size):
        if len(self.layers) > 0:
            input_dim = self.layers[-1].dim
        else:
            input_dim = self.input_dim

        self.layers.append(Layer(size, input_dim))

    def predict(self, input_data):
        return np.argmax(self.propagation(input_data))

        # Évalue la performance du réseau à partir d'un set d'exemples.
        # Retourne un nombre entre 0 et 1.

    def erreur(self, data, exemple):
        d = exemple
        p = data
        return max(np.matrix(d) - np.matrix(self.propagation(p)))




    def train(self, data, exemple, rate):

        deltas = []
        delta = self.output_delta(data, exemple)
        deltas.append(delta)


        nb_layers = len(self.layers)
        for l in reversed(range(nb_layers - 1)):
            layer = self.layers[l]
            next_layer = self.layers[l + 1]
            delta = sigmoid_prime(layer.z) * np.dot(next_layer.weights.transpose(), delta)
            deltas.append(delta)

        deltas = list(reversed(deltas))

        for layer in self.layers:
            index = self.layers.index(layer)
            prc_layer = None
            if index > 0:
                prc_layer = self.layers[index - 1]
            for j in range(layer.dim-1):
                for k in range(layer.input_size):
                    if index == 0:
                        layer.majweight(data[k] * deltas[index][j], rate, j, k)
                    else:
                        layer.majweight(prc_layer.a[k] * deltas[index][j], rate, j, k)
                layer.majbiase(deltas[index], rate)

    def output_delta(self, data, target):
        out = self.propagation(data)
        return out-target




if __name__ == '__main__':

    #Declaration variables
    chaine = 'ABCDEFGHIJVEKLMNOPQRSTUVWXYZ'
    liste_tuples = []
    random.seed()
    train_rate = 0.05

    net = Network(784)
    net.add_layer(10)
    net.add_layer(10)
    net.add_layer(26)  # à ne pas oublier 26 lettres alphabets

    for letter in chaine: # Parcours Alphabet
        #print(letter)
        for j in range(1,1000):
            data = cv2.imread('Exemples/letter'+ letter + str(j)+ '.png') # ouvre fichier
            tuple1 = (data, letter)
            if data is not None:
                data = cv2.resize(data, (int(28), int(28)))
                #print(data.shape)
                data = np.asarray(data)
                data2 = np.zeros((data.shape[0], data.shape[1]))
                #print(j)
                for i in range(data2.shape[0]):
                    for j in range(data2.shape[1]):
                        moy = np.sum(data[i][j]) / 3
                        data2[i][j] = moy / 255
                """vectorisation"""
                data2.shape = data2.shape[0] * data2.shape[1]
                tuple2 = (data2, letter)
                liste_tuples.append(tuple2)

    random.shuffle(liste_tuples)
    while i < len(liste_tuples):

        tps1 = time.clock()
        my_tuple3= liste_tuples[i]
        data3 = my_tuple3[0]
        my_letter = my_tuple3[1]
        exemple = np.zeros(26)
        exemple[ord(my_letter) - ord('A')] = 1
        # print('1ere propag')
        print(net.propagation(data3))
        for j in range(1, 10):
            net.train(data3, exemple, train_rate)

        print(net.propagation(data3))
        tps2 = time.clock()
        print("Temps execution :", (tps2 - tps1) / 60, "min.sec")
        print(my_letter)
        i += 1

    # save the model to disk
    filename = 'save.txt'
    pickle.dump(net, open(filename, 'wb'))
